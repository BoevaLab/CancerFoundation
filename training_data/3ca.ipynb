{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import logging\n",
    "import pathlib as pl\n",
    "import warnings\n",
    "import zipfile\n",
    "from argparse import ArgumentParser\n",
    "from itertools import product\n",
    "from typing import Union\n",
    "import scanpy as sc\n",
    "\n",
    "import anndata as anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "Pathlike = Union[str, pl.Path]\n",
    "\n",
    "SAMPLE = \"sample\"\n",
    "\n",
    "\n",
    "def download(url: str, fname: str, chunk_size=1024):\n",
    "    resp = requests.get(url, stream=True)\n",
    "    if resp:\n",
    "        total = int(resp.headers.get('content-length', 0))\n",
    "        with open(fname, 'wb') as file, tqdm(\n",
    "                desc=fname,\n",
    "                total=total,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for data in resp.iter_content(chunk_size=chunk_size):\n",
    "                size = file.write(data)\n",
    "                bar.update(size)\n",
    "    else:\n",
    "        raise ValueError(f\"Download from {url} failed\")\n",
    "\n",
    "class AnndataDir:\n",
    "    def __init__(self, path: Pathlike):\n",
    "        self.path = path\n",
    "        self.obs_path = self.get_obs_path()\n",
    "        self.var_path = self.get_var_path()\n",
    "        self.counts_path = self.get_counts_path()\n",
    "\n",
    "    @property\n",
    "    def counts_type(self):\n",
    "        return self.counts_path.name.split(\".\")[0]\n",
    "\n",
    "    def is_valid(self):\n",
    "        return all([self.obs_path, self.var_path, self.counts_path])\n",
    "\n",
    "    def get_obs_path(self):\n",
    "        for file in self.path.iterdir():\n",
    "            if file.name.endswith(\".csv\") and \"cell\" in file.name.lower():\n",
    "                return file\n",
    "\n",
    "    def get_var_path(self):\n",
    "        for file in self.path.iterdir():\n",
    "            if file.name.endswith(\".txt\") and \"gene\" in file.name.lower():\n",
    "                return file\n",
    "\n",
    "    def get_counts_path(self):\n",
    "        for file in self.path.iterdir():\n",
    "            if file.name.endswith((\".rds\")):\n",
    "                _LOGGER.info(f\"Found .rds in {str(self.path)}.\")\n",
    "            if file.name.endswith(\".mtx\"):\n",
    "                return file\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_counts(path) -> csr_matrix:\n",
    "        if path.name.endswith(\".mtx\"):\n",
    "            matrix = mmread(path).transpose()\n",
    "            matrix = matrix.tocsr()\n",
    "            matrix = matrix.astype(np.float32)\n",
    "            return matrix\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown counts type {path.name}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_object_columns_to_string(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        object_columns = df.select_dtypes(include='object').columns\n",
    "        if len(object_columns) > 0:\n",
    "            _LOGGER.info(f\"Casting {object_columns.to_list()} to string.\")\n",
    "            df[object_columns] = df[object_columns].fillna(\"\").astype(str)\n",
    "        return df\n",
    "\n",
    "    def create_anndata(self, metadata: pd.DataFrame) -> anndata.AnnData:\n",
    "        if not self.is_valid():\n",
    "            raise ValueError(\"Not a valid AnnDataDir.\")\n",
    "\n",
    "        _LOGGER.info(\"Reading obs.\")\n",
    "        obs = pd.read_csv(self.obs_path.open())\n",
    "        obs = self.merge_obs_metadata(obs, metadata)\n",
    "        obs = obs.set_index(\"cell_name\")\n",
    "        if obs.index.astype(str).str.isdigit().any():\n",
    "            obs.index = obs.index.astype(str) + \"_\" + obs[SAMPLE].astype(str)\n",
    "        obs = self._cast_object_columns_to_string(obs)\n",
    "        _LOGGER.info(\"Reading var.\")\n",
    "        var = pd.read_csv(self.var_path.open(), header=None, index_col=0)\n",
    "        var.index.name = \"genes\"\n",
    "        var = self._cast_object_columns_to_string(var)\n",
    "        _LOGGER.info(\"Reading counts. This might take a while.\")\n",
    "        counts = self._read_counts(self.counts_path.open())\n",
    "        counts_type = self.counts_type\n",
    "        uns = {\"count_type\": counts_type}\n",
    "        _LOGGER.info(\"Creating Anndata.\")\n",
    "        adata = anndata.AnnData(counts, obs=obs, var=var, uns=uns)\n",
    "        adata.obs_names_make_unique()\n",
    "        adata.var_names_make_unique()\n",
    "        return adata\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_obs_metadata(obs: pd.DataFrame, metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "        metadata = metadata.drop_duplicates()\n",
    "        if SAMPLE in metadata.columns:\n",
    "            if obs.index.isin(metadata[SAMPLE]).all():\n",
    "                _LOGGER.info(\"Merged the metadata on the cell level.\")\n",
    "                return obs.merge(metadata, how=\"left\", left_index=True,\n",
    "                                 right_on=SAMPLE, validate=\"m:1\")\n",
    "\n",
    "        for key_obs, key_metadata in product([SAMPLE, \"patient\"], [SAMPLE, \"patient\"]):\n",
    "            if key_obs in obs.columns and key_metadata in metadata.columns:\n",
    "                if obs[key_obs].isin(metadata[key_metadata]).all():\n",
    "                    _LOGGER.info(\n",
    "                        f\"Merging obs and metadata on {key_obs} and {key_metadata}.\"\n",
    "                    )\n",
    "                    return obs.merge(metadata, how=\"left\", left_on=key_obs,\n",
    "                                     right_on=key_metadata,\n",
    "                                     suffixes=(None, \"_metadata\"), validate=\"m:1\")\n",
    "\n",
    "        raise NotImplementedError(\"Did not find a way to merge the metadata and obs.\")\n",
    "\n",
    "\n",
    "class CCCAFetcher:\n",
    "    def __init__(self, meta_data: pd.DataFrame, download_dir: Pathlike,\n",
    "                 data_column: str = \"Data\", meta_data_column: str = \"Meta_data\"):\n",
    "        self.meta_data_column = meta_data_column\n",
    "        self.data_column = data_column\n",
    "        self.meta_data = meta_data\n",
    "        self.download_dir = pl.Path(download_dir)\n",
    "        self._make_download_dir()\n",
    "\n",
    "    def _make_download_dir(self) -> None:\n",
    "        if self.download_dir.is_dir():\n",
    "            warnings.warn(f\"Download dir {self.download_dir} already exists.\")\n",
    "        self.download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def fetch_datasets(self, force_download: bool = False) -> None:\n",
    "        _LOGGER.info(\"Start downloading datasets.\")\n",
    "        for name, url, metadata_url in self.meta_data[\n",
    "            [self.data_column, self.meta_data_column]].itertuples():\n",
    "            _LOGGER.info(f\"Downloading: {name}\")\n",
    "            self.fetch_data(url, force_download=force_download, file_extension=\"zip\")\n",
    "            self.fetch_data(metadata_url, force_download=force_download,\n",
    "                            file_extension=\"csv\")\n",
    "\n",
    "    def fetch_data(self, url: str, force_download: bool, file_extension: str) -> None:\n",
    "        data_path = self._url_to_data_path(url, file_extension)\n",
    "        if data_path.is_file() and not force_download:\n",
    "            return\n",
    "        download(url, str(data_path))\n",
    "\n",
    "    def _url_to_data_path(self, url: str, file_extension: str) -> pl.Path:\n",
    "        url_hash = self._hash_url(url)\n",
    "        data_path = self.download_dir.joinpath(f\"{url_hash}.{file_extension}\")\n",
    "        return data_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash_url(url: str) -> str:\n",
    "        hashed_url = hashlib.md5(url.encode(\"UTF-8\"))\n",
    "        return str(hashed_url.hexdigest())\n",
    "\n",
    "    def make_anndata(self, data_dir: Pathlike, index: int) -> None:\n",
    "        data_dir = pl.Path(data_dir)\n",
    "        data_dir.mkdir(exist_ok=True, parents=True)\n",
    "        row = self.meta_data.iloc[index]\n",
    "        name = row.name\n",
    "        url, metadata_url = row[[self.data_column, self.meta_data_column]]\n",
    "        _LOGGER.info(f\"Start generating AnnData for {name}\")\n",
    "        metadata_path = self._url_to_data_path(metadata_url, file_extension=\"csv\")\n",
    "        zpath = self._url_to_data_path(url, file_extension=\"zip\")\n",
    "        if not (metadata_path.is_file() and zpath.is_file()):\n",
    "            raise ValueError(\n",
    "                f\"Dataset {name} has not been downloaded run fetch_datasets.\")\n",
    "        _LOGGER.info(f\"Reading metadata from {metadata_path}.\")\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        _LOGGER.info(f\"Reading zipfile from {zpath}.\")\n",
    "\n",
    "        with zipfile.ZipFile(zpath) as zfile:\n",
    "            root = zipfile.Path(zfile)\n",
    "            adatadir = AnndataDir(root)\n",
    "            if adatadir.is_valid():\n",
    "                _LOGGER.info(\"Found data at the root level.\")\n",
    "                adata = adatadir.create_anndata(metadata)\n",
    "                data_path = data_dir.joinpath(f\"{name}.h5ad\")\n",
    "                if data_path.is_file():\n",
    "                    _LOGGER.info(f\"No data already exists.\")\n",
    "                    return\n",
    "                adata.write_h5ad(data_path)\n",
    "\n",
    "            else:\n",
    "                _LOGGER.info(\n",
    "                    \"Did not find data at the root level. Checking subdirs.\")\n",
    "                for subdir in root.iterdir():\n",
    "                    if not subdir.is_dir():\n",
    "                        continue\n",
    "                    adatadir = AnndataDir(subdir)\n",
    "                    data_path = data_dir.joinpath(f\"{name}_{subdir.name}.h5ad\")\n",
    "                    if data_path.is_file():\n",
    "                        _LOGGER.info(f\"No data already exists.\")\n",
    "                        continue\n",
    "\n",
    "                    if not adatadir.is_valid():\n",
    "                        _LOGGER.info(f\"No data found in {subdir.name}\")\n",
    "                        continue\n",
    "                    _LOGGER.info(f\"Found data in a {subdir.name}.\")\n",
    "                    adata = adatadir.create_anndata(metadata)\n",
    "                    adata.write_h5ad(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancerfoundation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
